{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dd95433-3914-435a-b65c-88e3ab83df9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "#         Deep Learning For Sequential Data and Computer Vision\n",
    "# ========================================================================\n",
    "#    Module: Advanced Time Series Models\n",
    "#    Topic: Introduction to GRU Networks\n",
    "#    \n",
    "#    Description:\n",
    "#    -----------\n",
    "#    This notebook introduces the Gated Recurrent Unit (GRU) architecture,\n",
    "#    covering its historical development, core concepts, mathematical \n",
    "#    formulation, and advantages over traditional RNNs. It includes\n",
    "#    visual representations of GRU components and information flow.\n",
    "#    \n",
    "#    Contents:\n",
    "#    1. Historical context and development of GRUs\n",
    "#    2. Core concepts: gates, memory cell mechanisms\n",
    "#    3. Mathematical formulation of GRU components\n",
    "#    4. Advantages over traditional RNNs\n",
    "#    5. Implementation considerations\n",
    "#    6. Visualizing GRU architecture with diagrams\n",
    "#    \n",
    "#    Objective:\n",
    "#    - Understand the motivation behind GRU development\n",
    "#    - Master the mathematical foundation of GRU operations\n",
    "#    - Visualize information flow through GRU components\n",
    "#    - Compare GRU advantages to traditional RNNs and LSTMs\n",
    "#    \n",
    "#    Author: Dr. Saad Laouadi\n",
    "#    Version: 1.0\n",
    "#    \n",
    "# ========================================================================\n",
    "#  ®Copyright Dr. Saad Laouadi, 2025. All rights reserved.\n",
    "# ========================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49581f7c-ad58-4d1e-9a5a-b978f49cddae",
   "metadata": {},
   "source": [
    "# Introduction to GRU Networks\n",
    "\n",
    "\n",
    "## Historical Context and Development\n",
    "\n",
    "The Gated Recurrent Unit (GRU) was introduced by **Cho et al.** in 2014 as part of their groundbreaking research in neural machine translation. This innovation came during a period when researchers were actively seeking solutions to the limitations of traditional Recurrent Neural Networks (RNNs), particularly the vanishing gradient problem that hindered their ability to learn long-term dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3832369-5541-49ab-b021-60fba4429e78",
   "metadata": {},
   "source": [
    "### Evolution from RNNs to GRUs\n",
    "\n",
    "Traditional RNNs process sequential data by maintaining a hidden state that gets updated at each time step according to the equation:\n",
    "\n",
    "$$ h_t = \\tanh(W_h h_{t-1} + W_x x_t + b) $$\n",
    "\n",
    "where:\n",
    "- $h_t$ is the hidden state at time t\n",
    "- $x_t$ is the input at time t\n",
    "- $W_h, W_x$ are weight matrices\n",
    "- $b$ is the bias vector\n",
    "\n",
    "While this architecture was revolutionary, it suffered from several limitations:\n",
    "\n",
    "1. **Vanishing Gradients**: During backpropagation through time, gradients could become exponentially small, making it difficult to learn long-term dependencies.\n",
    "\n",
    "2. **Information Retention**: The simple structure made it challenging to determine which information should be retained or discarded.\n",
    "\n",
    "3. **Training Stability**: The architecture often led to unstable training processes, especially for longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6407d75b-dce8-4736-b8bc-176a4ed92e1f",
   "metadata": {},
   "source": [
    "### The GRU Solution\n",
    "\n",
    "GRUs addressed these limitations by introducing gating mechanisms that control information flow, similar to LSTM but with a more streamlined architecture. This design offers several advantages:\n",
    "\n",
    "1. **Efficient Memory Usage**: GRUs require fewer parameters than LSTMs while maintaining comparable performance.\n",
    "2. **Better Gradient Flow**: The gating mechanisms help mitigate the vanishing gradient problem.\n",
    "3. **Adaptive Memory**: The network can learn to retain relevant information for varying time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312cde0-2be0-4a43-a6c5-34786d00ecca",
   "metadata": {},
   "source": [
    "## Core Concepts and Principles\n",
    "\n",
    "### Memory Cell Mechanism\n",
    "\n",
    "The GRU's memory cell is designed to adaptively capture dependencies of different time scales. Unlike LSTM's separate memory cell, GRU combines the memory cell with the hidden state, leading to a more compact representation:\n",
    "\n",
    "$$ h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$\n",
    "\n",
    "where:\n",
    "- $z_t$ is the update gate\n",
    "- $\\tilde{h}_t$ is the candidate activation\n",
    "- $\\odot$ represents element-wise multiplication\n",
    "\n",
    "### Gating Mechanisms\n",
    "\n",
    "GRU employs two main gates:\n",
    "\n",
    "1. **Update Gate** ($z_t$): Controls how much of the past information should be passed along to the future. The update gate is computed as:\n",
    "\n",
    "$$ z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z) $$\n",
    "\n",
    "2. **Reset Gate** ($r_t$): Determines how much of the past information to forget. The reset gate is computed as:\n",
    "\n",
    "$$ r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r) $$\n",
    "\n",
    "where:\n",
    "- $\\sigma$ is the sigmoid activation function\n",
    "- $W_z, W_r, U_z, U_r$ are weight matrices\n",
    "- $b_z, b_r$ are bias vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe48489-3c9b-4544-a83e-d07f1a66acf2",
   "metadata": {},
   "source": [
    "### Information Flow in GRU Cells\n",
    "\n",
    "The GRU processes information through three main steps:\n",
    "\n",
    "1. **Gate Computation**: Both update and reset gates are computed based on the current input and previous hidden state.\n",
    "\n",
    "2. **Candidate State Creation**: A candidate hidden state is computed using the reset gate:\n",
    "\n",
    "$$ \\tilde{h}_t = \\tanh(W_h x_t + U_h(r_t \\odot h_{t-1}) + b_h) $$\n",
    "\n",
    "3. **State Update**: The final hidden state is computed as a weighted combination of the previous hidden state and the candidate state, controlled by the update gate.\n",
    "\n",
    "$$ h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f5c4b-b281-43cd-8f5d-87b68eb6cbbc",
   "metadata": {},
   "source": [
    "## Advantages Over Traditional RNNs\n",
    "\n",
    "### 1. Gradient Flow Control\n",
    "\n",
    "GRUs provide better gradient flow through the network due to their gating mechanisms. The update gate allows gradients to flow through time steps with minimal decay when learning long-term dependencies.\n",
    "\n",
    "### 2. Adaptive Memory Duration\n",
    "\n",
    "The network can learn to:\n",
    "- Maintain long-term memories when necessary (high update gate values)\n",
    "- Quickly adapt to new inputs when needed (low update gate values)\n",
    "- Selectively combine past and present information (through reset gate modulation)\n",
    "\n",
    "### 3. Computational Efficiency\n",
    "\n",
    "The GRU architecture achieves comparable performance to LSTM with:\n",
    "- Fewer parameters (2 gates instead of 3)\n",
    "- Simpler computations (combined hidden state and memory)\n",
    "- More efficient training process\n",
    "\n",
    "## Implementation Considerations\n",
    "\n",
    "When implementing GRUs, several key factors should be considered:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Gates should be initialized with a bias towards being open (small positive values)\n",
    "   - Weight matrices should be initialized carefully to ensure stable training\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - Sigmoid ($\\sigma$) for gates (output range [0,1])\n",
    "   - Tanh for candidate state (output range [-1,1])\n",
    "\n",
    "3. **Dimensionality**:\n",
    "   The hidden state dimension is a crucial hyperparameter that affects:\n",
    "   - Model capacity\n",
    "   - Computational requirements\n",
    "   - Memory usage\n",
    "\n",
    "## Learning Objectives Review\n",
    "\n",
    "After studying this section, you should be able to:\n",
    "- Explain the historical context and motivation behind GRU development\n",
    "- Describe the core components of a GRU cell\n",
    "- Understand the mathematical formulation of GRU gates\n",
    "- Compare GRU advantages over traditional RNNs\n",
    "- Identify key implementation considerations\n",
    "\n",
    "## Key Equations Summary\n",
    "\n",
    "1. Update Gate:\n",
    "$$ z_t = \\sigma(W_z x_t + U_z h_{t-1} + b_z) $$\n",
    "\n",
    "2. Reset Gate:\n",
    "$$ r_t = \\sigma(W_r x_t + U_r h_{t-1} + b_r) $$\n",
    "\n",
    "3. Candidate Hidden State:\n",
    "$$ \\tilde{h}_t = \\tanh(W_h x_t + U_h(r_t \\odot h_{t-1}) + b_h) $$\n",
    "\n",
    "4. Final Hidden State:\n",
    "$$ h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$\n",
    "\n",
    "These equations form the foundation for understanding GRU behavior and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfa3209-1b8a-4d28-a487-adb11760d3df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext nb_js_diagrammers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea5ff867-a764-4142-a9a0-c898ceae763e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            graph TD\n",
       "    X[Input x_t] --&gt; U1[Update Gate]\n",
       "    X --&gt; R1[Reset Gate]\n",
       "    X --&gt; C1[Candidate State]\n",
       "    H[Previous h_t-1] --&gt; U1\n",
       "    H --&gt; R1\n",
       "    H --&gt; RH[Reset × Hidden]\n",
       "    \n",
       "    U1 --&gt; U2[z_t]\n",
       "    R1 --&gt; R2[r_t]\n",
       "    \n",
       "    R2 --&gt; RH\n",
       "    RH --&gt; C1\n",
       "    \n",
       "    C1 --&gt; C2[h̃_t]\n",
       "    \n",
       "    U2 --&gt; UH1[Update × Candidate]\n",
       "    U2 --&gt; UH2[1-Update × Previous]\n",
       "    \n",
       "    C2 --&gt; UH1\n",
       "    H --&gt; UH2\n",
       "    UH1 --&gt; FH[New h_t]\n",
       "    UH2 --&gt; FH\n",
       "    \n",
       "    classDef input fill:#d9f2ff,stroke:#333,stroke-width:2px\n",
       "    classDef gate fill:#ffedd9,stroke:#333,stroke-width:2px\n",
       "    classDef operation fill:#f2f2f2,stroke:#333,stroke-width:2px\n",
       "    classDef state fill:#e6ffe6,stroke:#333,stroke-width:2px\n",
       "    \n",
       "    class X,H input\n",
       "    class U1,R1,U2,R2 gate\n",
       "    class RH,UH1,UH2 operation\n",
       "    class C1,C2,FH state\n",
       "    \n",
       "    subgraph Gates\n",
       "        U1\n",
       "        R1\n",
       "    end\n",
       "    subgraph Operations\n",
       "        RH\n",
       "        UH1\n",
       "        UH2\n",
       "    end\n",
       "    subgraph States\n",
       "        C1\n",
       "        C2\n",
       "        FH\n",
       "    end\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"1000\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x10627b210>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 1000\n",
    "graph TD\n",
    "    X[Input x_t] --> U1[Update Gate]\n",
    "    X --> R1[Reset Gate]\n",
    "    X --> C1[Candidate State]\n",
    "    H[Previous h_t-1] --> U1\n",
    "    H --> R1\n",
    "    H --> RH[Reset × Hidden]\n",
    "    \n",
    "    U1 --> U2[z_t]\n",
    "    R1 --> R2[r_t]\n",
    "    \n",
    "    R2 --> RH\n",
    "    RH --> C1\n",
    "    \n",
    "    C1 --> C2[h̃_t]\n",
    "    \n",
    "    U2 --> UH1[Update × Candidate]\n",
    "    U2 --> UH2[1-Update × Previous]\n",
    "    \n",
    "    C2 --> UH1\n",
    "    H --> UH2\n",
    "    UH1 --> FH[New h_t]\n",
    "    UH2 --> FH\n",
    "    \n",
    "    classDef input fill:#d9f2ff,stroke:#333,stroke-width:2px\n",
    "    classDef gate fill:#ffedd9,stroke:#333,stroke-width:2px\n",
    "    classDef operation fill:#f2f2f2,stroke:#333,stroke-width:2px\n",
    "    classDef state fill:#e6ffe6,stroke:#333,stroke-width:2px\n",
    "    \n",
    "    class X,H input\n",
    "    class U1,R1,U2,R2 gate\n",
    "    class RH,UH1,UH2 operation\n",
    "    class C1,C2,FH state\n",
    "    \n",
    "    subgraph Gates\n",
    "        U1\n",
    "        R1\n",
    "    end\n",
    "    subgraph Operations\n",
    "        RH\n",
    "        UH1\n",
    "        UH2\n",
    "    end\n",
    "    subgraph States\n",
    "        C1\n",
    "        C2\n",
    "        FH\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5585f219-f87b-4c7f-8dbc-927a14ebeccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            graph TD\n",
       "    X[Input x_t] --&gt; U1[Update Gate]\n",
       "    X --&gt; R1[Reset Gate]\n",
       "    X --&gt; C1[Candidate State]\n",
       "    H[Previous h_t-1] --&gt; U1\n",
       "    H --&gt; R1\n",
       "    H --&gt; RH[Reset × Hidden]\n",
       "    \n",
       "    U1 --&gt; U2[z_t]\n",
       "    R1 --&gt; R2[r_t]\n",
       "    \n",
       "    R2 --&gt; RH\n",
       "    RH --&gt; C1\n",
       "    \n",
       "    C1 --&gt; C2[h̃_t]\n",
       "    \n",
       "    U2 --&gt; UH1[Update × Candidate]\n",
       "    U2 --&gt; UH2[1-Update × Previous]\n",
       "    \n",
       "    C2 --&gt; UH1\n",
       "    H --&gt; UH2\n",
       "    UH1 --&gt; FH[New h_t]\n",
       "    UH2 --&gt; FH\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"1000\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x1062fcf10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 1000\n",
    "graph TD\n",
    "    X[Input x_t] --> U1[Update Gate]\n",
    "    X --> R1[Reset Gate]\n",
    "    X --> C1[Candidate State]\n",
    "    H[Previous h_t-1] --> U1\n",
    "    H --> R1\n",
    "    H --> RH[Reset × Hidden]\n",
    "    \n",
    "    U1 --> U2[z_t]\n",
    "    R1 --> R2[r_t]\n",
    "    \n",
    "    R2 --> RH\n",
    "    RH --> C1\n",
    "    \n",
    "    C1 --> C2[h̃_t]\n",
    "    \n",
    "    U2 --> UH1[Update × Candidate]\n",
    "    U2 --> UH2[1-Update × Previous]\n",
    "    \n",
    "    C2 --> UH1\n",
    "    H --> UH2\n",
    "    UH1 --> FH[New h_t]\n",
    "    UH2 --> FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc04e7-d418-4526-aae5-60a320b399ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

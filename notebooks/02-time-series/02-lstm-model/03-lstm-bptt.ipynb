{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543b200f-8e10-4824-8a29-afe3cc152f0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LSTM BACKPROPAGATION THROUGH TIME (BPTT) DEMONSTRATION\n",
      "================================================================================\n",
      "Training on 8 sequences with lookback=2 for 3 epochs\n",
      "Learning rate: 0.01\n",
      "================================================================================\n",
      "\n",
      "============================== EPOCH 1 ==============================\n",
      "\n",
      "--- FORWARD PASS: Sequence 1 (Days 1-2) ---\n",
      "  Processed Day 1 - Hidden state norm: 0.1849\n",
      "  Processed Day 2 - Hidden state norm: 0.2504\n",
      "  Prediction for Day 3: 0.0028 kWh (Actual: 34.4384 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 2 (Days 2-3) ---\n",
      "  Processed Day 2 - Hidden state norm: 0.2774\n",
      "  Processed Day 3 - Hidden state norm: 0.2853\n",
      "  Prediction for Day 4: 0.0030 kWh (Actual: 48.3551 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 3 (Days 3-4) ---\n",
      "  Processed Day 3 - Hidden state norm: 0.2884\n",
      "  Processed Day 4 - Hidden state norm: 0.2823\n",
      "  Prediction for Day 5: 0.0031 kWh (Actual: 50.7292 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 4 (Days 4-5) ---\n",
      "  Processed Day 4 - Hidden state norm: 0.2798\n",
      "  Processed Day 5 - Hidden state norm: 0.2622\n",
      "  Prediction for Day 6: 0.0032 kWh (Actual: 58.0393 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 5 (Days 5-6) ---\n",
      "  Processed Day 5 - Hidden state norm: 0.2525\n",
      "  Processed Day 6 - Hidden state norm: 0.2390\n",
      "  Prediction for Day 7: 0.0034 kWh (Actual: 57.4861 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 6 (Days 6-7) ---\n",
      "  Processed Day 6 - Hidden state norm: 0.2338\n",
      "  Processed Day 7 - Hidden state norm: 0.2474\n",
      "  Prediction for Day 8: 0.0035 kWh (Actual: 45.2372 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 7 (Days 7-8) ---\n",
      "  Processed Day 7 - Hidden state norm: 0.2538\n",
      "  Processed Day 8 - Hidden state norm: 0.2652\n",
      "  Prediction for Day 9: 0.0035 kWh (Actual: 27.9426 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 8 (Days 8-9) ---\n",
      "  Processed Day 8 - Hidden state norm: 0.2701\n",
      "  Processed Day 9 - Hidden state norm: 0.2828\n",
      "  Prediction for Day 10: 0.0033 kWh (Actual: 20.3628 kWh)\n",
      "\n",
      "Epoch 1 Loss: 2001.3153\n",
      "\n",
      "--- BACKPROPAGATION THROUGH TIME ---\n",
      "\n",
      "  Backpropagating Sequence 8\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0013\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0011\n",
      "\n",
      "  Backpropagating Sequence 7\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0019\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0015\n",
      "\n",
      "  Backpropagating Sequence 6\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0032\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0026\n",
      "\n",
      "  Backpropagating Sequence 5\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0045\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0032\n",
      "\n",
      "  Backpropagating Sequence 4\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0043\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0032\n",
      "\n",
      "  Backpropagating Sequence 3\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0034\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0028\n",
      "\n",
      "  Backpropagating Sequence 2\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0031\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0027\n",
      "\n",
      "  Backpropagating Sequence 1\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0022\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0019\n",
      "\n",
      "  Applied gradients with learning rate 0.01 and clipping threshold 5.0\n",
      "\n",
      "  Weight Update Summary:\n",
      "+-----------------+--------------------+\n",
      "| Weight Matrix   |   Change Magnitude |\n",
      "+=================+====================+\n",
      "| W_f             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_i             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_c             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_o             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_y             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "\n",
      "============================== EPOCH 2 ==============================\n",
      "\n",
      "--- FORWARD PASS: Sequence 1 (Days 1-2) ---\n",
      "  Processed Day 1 - Hidden state norm: 0.8137\n",
      "  Processed Day 2 - Hidden state norm: 1.0908\n",
      "  Prediction for Day 3: 6.9253 kWh (Actual: 34.4384 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 2 (Days 2-3) ---\n",
      "  Processed Day 2 - Hidden state norm: 1.1825\n",
      "  Processed Day 3 - Hidden state norm: 1.1961\n",
      "  Prediction for Day 4: 6.9325 kWh (Actual: 48.3551 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 3 (Days 3-4) ---\n",
      "  Processed Day 3 - Hidden state norm: 1.2102\n",
      "  Processed Day 4 - Hidden state norm: 1.2041\n",
      "  Prediction for Day 5: 6.9331 kWh (Actual: 50.7292 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 4 (Days 4-5) ---\n",
      "  Processed Day 4 - Hidden state norm: 1.2069\n",
      "  Processed Day 5 - Hidden state norm: 1.1578\n",
      "  Prediction for Day 6: 6.9300 kWh (Actual: 58.0393 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 5 (Days 5-6) ---\n",
      "  Processed Day 5 - Hidden state norm: 1.1533\n",
      "  Processed Day 6 - Hidden state norm: 1.1342\n",
      "  Prediction for Day 7: 6.9284 kWh (Actual: 57.4861 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 6 (Days 6-7) ---\n",
      "  Processed Day 6 - Hidden state norm: 1.1303\n",
      "  Processed Day 7 - Hidden state norm: 1.1615\n",
      "  Prediction for Day 8: 6.9302 kWh (Actual: 45.2372 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 7 (Days 7-8) ---\n",
      "  Processed Day 7 - Hidden state norm: 1.1643\n",
      "  Processed Day 8 - Hidden state norm: 1.1836\n",
      "  Prediction for Day 9: 6.9317 kWh (Actual: 27.9426 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 8 (Days 8-9) ---\n",
      "  Processed Day 8 - Hidden state norm: 1.1867\n",
      "  Processed Day 9 - Hidden state norm: 1.2274\n",
      "  Prediction for Day 10: 6.9347 kWh (Actual: 20.3628 kWh)\n",
      "\n",
      "Epoch 2 Loss: 1456.0433\n",
      "\n",
      "--- BACKPROPAGATION THROUGH TIME ---\n",
      "\n",
      "  Backpropagating Sequence 8\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0070\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0006\n",
      "\n",
      "  Backpropagating Sequence 7\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0115\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0010\n",
      "\n",
      "  Backpropagating Sequence 6\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0214\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0021\n",
      "\n",
      "  Backpropagating Sequence 5\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0296\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0028\n",
      "\n",
      "  Backpropagating Sequence 4\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0298\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0023\n",
      "\n",
      "  Backpropagating Sequence 3\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0238\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0019\n",
      "\n",
      "  Backpropagating Sequence 2\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0216\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0020\n",
      "\n",
      "  Backpropagating Sequence 1\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0124\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0017\n",
      "\n",
      "  Applied gradients with learning rate 0.01 and clipping threshold 5.0\n",
      "\n",
      "  Weight Update Summary:\n",
      "+-----------------+--------------------+\n",
      "| Weight Matrix   |   Change Magnitude |\n",
      "+=================+====================+\n",
      "| W_f             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_i             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_c             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_o             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "| W_y             |               0.05 |\n",
      "+-----------------+--------------------+\n",
      "\n",
      "============================== EPOCH 3 ==============================\n",
      "\n",
      "--- FORWARD PASS: Sequence 1 (Days 1-2) ---\n",
      "  Processed Day 1 - Hidden state norm: 1.2040\n",
      "  Processed Day 2 - Hidden state norm: 1.5500\n",
      "  Prediction for Day 3: 12.7742 kWh (Actual: 34.4384 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 2 (Days 2-3) ---\n",
      "  Processed Day 2 - Hidden state norm: 1.6208\n",
      "  Processed Day 3 - Hidden state norm: 1.6216\n",
      "  Prediction for Day 4: 12.7828 kWh (Actual: 48.3551 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 3 (Days 3-4) ---\n",
      "  Processed Day 3 - Hidden state norm: 1.6253\n",
      "  Processed Day 4 - Hidden state norm: 1.6174\n",
      "  Prediction for Day 5: 12.7825 kWh (Actual: 50.7292 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 4 (Days 4-5) ---\n",
      "  Processed Day 4 - Hidden state norm: 1.6178\n",
      "  Processed Day 5 - Hidden state norm: 1.5768\n",
      "  Prediction for Day 6: 12.7781 kWh (Actual: 58.0393 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 5 (Days 5-6) ---\n",
      "  Processed Day 5 - Hidden state norm: 1.5769\n",
      "  Processed Day 6 - Hidden state norm: 1.5631\n",
      "  Prediction for Day 7: 12.7766 kWh (Actual: 57.4861 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 6 (Days 6-7) ---\n",
      "  Processed Day 6 - Hidden state norm: 1.5632\n",
      "  Processed Day 7 - Hidden state norm: 1.5916\n",
      "  Prediction for Day 8: 12.7797 kWh (Actual: 45.2372 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 7 (Days 7-8) ---\n",
      "  Processed Day 7 - Hidden state norm: 1.5916\n",
      "  Processed Day 8 - Hidden state norm: 1.6051\n",
      "  Prediction for Day 9: 12.7812 kWh (Actual: 27.9426 kWh)\n",
      "\n",
      "--- FORWARD PASS: Sequence 8 (Days 8-9) ---\n",
      "  Processed Day 8 - Hidden state norm: 1.6051\n",
      "  Processed Day 9 - Hidden state norm: 1.6333\n",
      "  Prediction for Day 10: 12.7842 kWh (Actual: 20.3628 kWh)\n",
      "\n",
      "Epoch 3 Loss: 1070.3750\n",
      "\n",
      "--- BACKPROPAGATION THROUGH TIME ---\n",
      "\n",
      "  Backpropagating Sequence 8\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0025\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0000\n",
      "\n",
      "  Backpropagating Sequence 7\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0063\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0001\n",
      "\n",
      "  Backpropagating Sequence 6\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0149\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0001\n",
      "\n",
      "  Backpropagating Sequence 5\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0243\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0002\n",
      "\n",
      "  Backpropagating Sequence 4\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0226\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0002\n",
      "\n",
      "  Backpropagating Sequence 3\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0142\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0001\n",
      "\n",
      "  Backpropagating Sequence 2\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0123\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0002\n",
      "\n",
      "  Backpropagating Sequence 1\n",
      "    Backpropagated timestep 2 - Gradient norm: 0.0067\n",
      "    Backpropagated timestep 1 - Gradient norm: 0.0005\n",
      "\n",
      "  Applied gradients with learning rate 0.01 and clipping threshold 5.0\n",
      "\n",
      "  Weight Update Summary:\n",
      "+-----------------+--------------------+\n",
      "| Weight Matrix   |   Change Magnitude |\n",
      "+=================+====================+\n",
      "| W_f             |           0.05     |\n",
      "+-----------------+--------------------+\n",
      "| W_i             |           0.05     |\n",
      "+-----------------+--------------------+\n",
      "| W_c             |           0.035725 |\n",
      "+-----------------+--------------------+\n",
      "| W_o             |           0.05     |\n",
      "+-----------------+--------------------+\n",
      "| W_y             |           0.05     |\n",
      "+-----------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Loss Progression:\n",
      "+---------+---------+\n",
      "|   Epoch |    Loss |\n",
      "+=========+=========+\n",
      "|       1 | 2001.32 |\n",
      "+---------+---------+\n",
      "|       2 | 1456.04 |\n",
      "+---------+---------+\n",
      "|       3 | 1070.38 |\n",
      "+---------+---------+\n",
      "\n",
      "Weight Change Magnitudes:\n",
      "+---------+-------+-------+----------+-------+-------+\n",
      "|   Epoch |   W_f |   W_i |      W_c |   W_o |   W_y |\n",
      "+=========+=======+=======+==========+=======+=======+\n",
      "|       1 |  0.05 |  0.05 | 0.05     |  0.05 |  0.05 |\n",
      "+---------+-------+-------+----------+-------+-------+\n",
      "|       2 |  0.05 |  0.05 | 0.05     |  0.05 |  0.05 |\n",
      "+---------+-------+-------+----------+-------+-------+\n",
      "|       3 |  0.05 |  0.05 | 0.035725 |  0.05 |  0.05 |\n",
      "+---------+-------+-------+----------+-------+-------+\n",
      "\n",
      "Weight Comparison (Initial vs Final):\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "| Weight Matrix   |   Initial Norm |   Final Norm | Relative Change   |\n",
      "+=================+================+==============+===================+\n",
      "| W_f             |       0.053668 |     0.115513 | 211.74%           |\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "| W_i             |       0.044763 |     0.109582 | 225.76%           |\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "| W_c             |       0.04967  |     0.122837 | 200.03%           |\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "| W_o             |       0.048857 |     0.135096 | 287.62%           |\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "| W_y             |       0.030126 |     0.167566 | 489.06%           |\n",
      "+-----------------+----------------+--------------+-------------------+\n",
      "\n",
      "Backpropagation Through Time (BPTT) Key Insights:\n",
      "1. BPTT involves unfolding the network through time and computing gradients at each timestep\n",
      "2. The gradients from later timesteps flow back to earlier timesteps (temporal dependency)\n",
      "3. The cell state (C_t) acts as a memory mechanism, preserving information across timesteps\n",
      "4. The gates (f_t, i_t, o_t) control information flow and have their own gradient paths\n",
      "5. Gradient clipping helps prevent the exploding gradient problem in recurrent networks\n",
      "6. The learning process adjusts weights to minimize the prediction error over time\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate our sample data\n",
    "temperature = np.array([18.2, 19.5, 20.1, 22.4, 23.8, 25.0, 23.2, 21.5, 19.8, 17.5])\n",
    "humidity = np.array([65.2, 62.8, 58.5, 55.0, 45.2, 42.1, 48.5, 52.3, 60.5, 67.8])\n",
    "wind_speed = np.array([5.2, 6.8, 8.5, 10.2, 12.5, 14.8, 13.2, 11.5, 9.2, 6.5])\n",
    "X = np.column_stack((temperature, humidity, wind_speed))\n",
    "\n",
    "# Target: power consumption (kWh)\n",
    "y = 2.5 * temperature - 0.5 * humidity + 1.2 * wind_speed + np.random.normal(0, 5, 10)\n",
    "\n",
    "# Create sequences with lookback of 2 (predict the step after the sequence)\n",
    "def create_sequences(X, y, lookback=2):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - lookback):\n",
    "        X_seq.append(X[i:i+lookback])\n",
    "        y_seq.append(y[i+lookback])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(X, y, lookback=2)\n",
    "\n",
    "# Define LSTM dimensions\n",
    "n_features = 3  # Temperature, humidity, wind speed\n",
    "n_hidden = 4    # Number of LSTM units\n",
    "n_output = 1    # Power consumption prediction\n",
    "\n",
    "# Initialize weight matrices and biases\n",
    "# For simplicity, we'll use small random values\n",
    "W_f = np.random.randn(n_hidden, n_features + n_hidden) * 0.01\n",
    "b_f = np.zeros(n_hidden)\n",
    "\n",
    "W_i = np.random.randn(n_hidden, n_features + n_hidden) * 0.01\n",
    "b_i = np.zeros(n_hidden)\n",
    "\n",
    "W_c = np.random.randn(n_hidden, n_features + n_hidden) * 0.01\n",
    "b_c = np.zeros(n_hidden)\n",
    "\n",
    "W_o = np.random.randn(n_hidden, n_features + n_hidden) * 0.01\n",
    "b_o = np.zeros(n_hidden)\n",
    "\n",
    "W_y = np.random.randn(n_output, n_hidden) * 0.01\n",
    "b_y = np.zeros(n_output)\n",
    "\n",
    "# Keep copies of initial weights for comparison\n",
    "W_f_initial = W_f.copy()\n",
    "W_i_initial = W_i.copy() \n",
    "W_c_initial = W_c.copy()\n",
    "W_o_initial = W_o.copy()\n",
    "W_y_initial = W_y.copy()\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid_derivative(values):\n",
    "    return values * (1 - values)\n",
    "\n",
    "def tanh_derivative(values):\n",
    "    return 1 - values**2\n",
    "\n",
    "# Loss function (MSE) and its derivative\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "# Loss function (MSE) and its derivative\n",
    "def mse_loss(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def mse_derivative(y_pred, y_true):\n",
    "    # Handle both scalar and array cases\n",
    "    if np.isscalar(y_true) or (hasattr(y_true, 'shape') and len(y_true.shape) == 0):\n",
    "        return 2 * (y_pred - y_true)\n",
    "    else:\n",
    "        return 2 * (y_pred - y_true) / len(y_true)\n",
    "\n",
    "# Learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Track metrics across epochs\n",
    "epoch_losses = []\n",
    "weight_changes = []\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LSTM BACKPROPAGATION THROUGH TIME (BPTT) DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training on {len(X_sequences)} sequences with lookback=2 for 3 epochs\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"\\n{'=' * 30} EPOCH {epoch+1} {'=' * 30}\")\n",
    "    \n",
    "    # Initialize states for this epoch\n",
    "    h_t = np.zeros(n_hidden)\n",
    "    C_t = np.zeros(n_hidden)\n",
    "    \n",
    "    epoch_predictions = []\n",
    "    epoch_true_values = []\n",
    "    \n",
    "    # Forward pass: Store all intermediate values needed for backpropagation\n",
    "    forward_states = []\n",
    "    \n",
    "    for seq_idx, sequence in enumerate(X_sequences):\n",
    "        print(f\"\\n--- FORWARD PASS: Sequence {seq_idx+1} (Days {seq_idx+1}-{seq_idx+2}) ---\")\n",
    "        \n",
    "        # Store all intermediate values for this sequence\n",
    "        sequence_cache = {'x': [], 'combined': [], 'f_gate': [], 'i_gate': [], \n",
    "                         'c_tilde': [], 'o_gate': [], 'c_state': [], 'h_state': []}\n",
    "        \n",
    "        # Store initial states for this sequence\n",
    "        h_prev = h_t.copy()\n",
    "        C_prev = C_t.copy()\n",
    "        sequence_cache['h_state'].append(h_prev)\n",
    "        sequence_cache['c_state'].append(C_prev)\n",
    "        \n",
    "        # Process each time step in the sequence (forward pass)\n",
    "        for t in range(len(sequence)):\n",
    "            x_t = sequence[t]\n",
    "            day_num = seq_idx + t + 1\n",
    "            \n",
    "            # Store input\n",
    "            sequence_cache['x'].append(x_t)\n",
    "            \n",
    "            # Concatenate input with previous hidden state\n",
    "            combined = np.concatenate([x_t, h_t])\n",
    "            sequence_cache['combined'].append(combined)\n",
    "            \n",
    "            # Calculate gate activations\n",
    "            f_t = sigmoid(np.dot(W_f, combined) + b_f)\n",
    "            i_t = sigmoid(np.dot(W_i, combined) + b_i)\n",
    "            c_tilde = tanh(np.dot(W_c, combined) + b_c)\n",
    "            o_t = sigmoid(np.dot(W_o, combined) + b_o)\n",
    "            \n",
    "            # Store gate values\n",
    "            sequence_cache['f_gate'].append(f_t)\n",
    "            sequence_cache['i_gate'].append(i_t)\n",
    "            sequence_cache['c_tilde'].append(c_tilde)\n",
    "            sequence_cache['o_gate'].append(o_t)\n",
    "            \n",
    "            # Update cell state\n",
    "            C_t = f_t * C_t + i_t * c_tilde\n",
    "            \n",
    "            # Update hidden state\n",
    "            h_t = o_t * tanh(C_t)\n",
    "            \n",
    "            # Store updated states\n",
    "            sequence_cache['c_state'].append(C_t.copy())\n",
    "            sequence_cache['h_state'].append(h_t.copy())\n",
    "            \n",
    "            print(f\"  Processed Day {day_num} - Hidden state norm: {np.linalg.norm(h_t):.4f}\")\n",
    "        \n",
    "        # Make prediction for this sequence\n",
    "        y_pred = np.dot(W_y, h_t) + b_y\n",
    "        \n",
    "        # Store prediction and actual value\n",
    "        epoch_predictions.append(y_pred[0])\n",
    "        epoch_true_values.append(y_sequences[seq_idx])\n",
    "        \n",
    "        # Store for backpropagation\n",
    "        sequence_cache['y_pred'] = y_pred\n",
    "        sequence_cache['y_true'] = y_sequences[seq_idx]\n",
    "        \n",
    "        print(f\"  Prediction for Day {seq_idx+3}: {y_pred[0]:.4f} kWh (Actual: {y_sequences[seq_idx]:.4f} kWh)\")\n",
    "        \n",
    "        # Store all cache for this sequence\n",
    "        forward_states.append(sequence_cache)\n",
    "    \n",
    "    # Calculate epoch loss\n",
    "    epoch_loss = mse_loss(np.array(epoch_predictions), y_sequences)\n",
    "    epoch_losses.append(epoch_loss)\n",
    "    print(f\"\\nEpoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- BACKPROPAGATION THROUGH TIME ---\")\n",
    "    \n",
    "    # Initialize gradients\n",
    "    dW_f = np.zeros_like(W_f)\n",
    "    db_f = np.zeros_like(b_f)\n",
    "    dW_i = np.zeros_like(W_i)\n",
    "    db_i = np.zeros_like(b_i)\n",
    "    dW_c = np.zeros_like(W_c)\n",
    "    db_c = np.zeros_like(b_c)\n",
    "    dW_o = np.zeros_like(W_o)\n",
    "    db_o = np.zeros_like(b_o)\n",
    "    dW_y = np.zeros_like(W_y)\n",
    "    db_y = np.zeros_like(b_y)\n",
    "    \n",
    "    # Process sequences in reverse order for BPTT\n",
    "    for seq_idx in range(len(forward_states)-1, -1, -1):\n",
    "        cache = forward_states[seq_idx]\n",
    "        \n",
    "        print(f\"\\n  Backpropagating Sequence {seq_idx+1}\")\n",
    "        \n",
    "        # Initialize gradient of the loss with respect to the output\n",
    "        dy = mse_derivative(cache['y_pred'], cache['y_true'])  # Shape: (1,)\n",
    "        \n",
    "        # Gradient for the output layer\n",
    "        dW_y += np.outer(dy, cache['h_state'][-1])  # Gradient for W_y\n",
    "        db_y += dy  # Gradient for b_y\n",
    "        \n",
    "        # Initial gradient with respect to hidden state\n",
    "        dh_next = np.dot(W_y.T, dy)  # Shape: (n_hidden,)\n",
    "        dC_next = np.zeros_like(cache['c_state'][0])  # Shape: (n_hidden,)\n",
    "        \n",
    "        # Backpropagate through time steps in reverse order\n",
    "        for t in range(len(cache['x'])-1, -1, -1):\n",
    "            # Current hidden state and cell state\n",
    "            h_t = cache['h_state'][t+1]  # +1 because we stored initial state at index 0\n",
    "            C_t = cache['c_state'][t+1]\n",
    "            C_prev = cache['c_state'][t]\n",
    "            \n",
    "            # Gate values\n",
    "            o_t = cache['o_gate'][t]\n",
    "            f_t = cache['f_gate'][t]\n",
    "            i_t = cache['i_gate'][t]\n",
    "            c_tilde = cache['c_tilde'][t]\n",
    "            \n",
    "            # Backpropagate through the tanh in h_t = o_t * tanh(C_t)\n",
    "            dC_part = dh_next * o_t * tanh_derivative(tanh(C_t))\n",
    "            dC = dC_next + dC_part\n",
    "            \n",
    "            # Backpropagate through the gates\n",
    "            do = dh_next * tanh(C_t) * sigmoid_derivative(o_t)\n",
    "            df = dC * C_prev * sigmoid_derivative(f_t)\n",
    "            di = dC * c_tilde * sigmoid_derivative(i_t)\n",
    "            dc_tilde = dC * i_t * tanh_derivative(c_tilde)\n",
    "            \n",
    "            # Combined input\n",
    "            combined = cache['combined'][t]\n",
    "            \n",
    "            # Gradient for gate weights and biases\n",
    "            dW_f += np.outer(df, combined)\n",
    "            db_f += df\n",
    "            dW_i += np.outer(di, combined)\n",
    "            db_i += di\n",
    "            dW_c += np.outer(dc_tilde, combined)\n",
    "            db_c += dc_tilde\n",
    "            dW_o += np.outer(do, combined)\n",
    "            db_o += do\n",
    "            \n",
    "            # Gradient with respect to the combined input\n",
    "            dcombined = (np.dot(W_f.T, df) + \n",
    "                        np.dot(W_i.T, di) + \n",
    "                        np.dot(W_c.T, dc_tilde) + \n",
    "                        np.dot(W_o.T, do))\n",
    "            \n",
    "            # Split gradient between x and h components\n",
    "            dx = dcombined[:n_features]\n",
    "            dh_prev = dcombined[n_features:]\n",
    "            \n",
    "            # Gradient for next backward step\n",
    "            dh_next = dh_prev\n",
    "            dC_next = dC * f_t\n",
    "            \n",
    "            print(f\"    Backpropagated timestep {t+1} - Gradient norm: {np.linalg.norm(dh_next):.4f}\")\n",
    "    \n",
    "    # Apply gradients with clipping\n",
    "    clip_threshold = 5.0\n",
    "    \n",
    "    # Function to clip gradients\n",
    "    def clip_gradient(grad):\n",
    "        norm = np.linalg.norm(grad)\n",
    "        if norm > clip_threshold:\n",
    "            return grad * (clip_threshold / norm)\n",
    "        return grad\n",
    "    \n",
    "    # Clip and apply gradients\n",
    "    dW_f = clip_gradient(dW_f)\n",
    "    dW_i = clip_gradient(dW_i)\n",
    "    dW_c = clip_gradient(dW_c)\n",
    "    dW_o = clip_gradient(dW_o)\n",
    "    dW_y = clip_gradient(dW_y)\n",
    "    \n",
    "    # Record weight changes for this epoch\n",
    "    weight_changes_epoch = {\n",
    "        'W_f': np.linalg.norm(dW_f * learning_rate),\n",
    "        'W_i': np.linalg.norm(dW_i * learning_rate),\n",
    "        'W_c': np.linalg.norm(dW_c * learning_rate),\n",
    "        'W_o': np.linalg.norm(dW_o * learning_rate),\n",
    "        'W_y': np.linalg.norm(dW_y * learning_rate)\n",
    "    }\n",
    "    weight_changes.append(weight_changes_epoch)\n",
    "    \n",
    "    # Update weights\n",
    "    W_f -= learning_rate * dW_f\n",
    "    b_f -= learning_rate * db_f\n",
    "    W_i -= learning_rate * dW_i\n",
    "    b_i -= learning_rate * db_i\n",
    "    W_c -= learning_rate * dW_c\n",
    "    b_c -= learning_rate * db_c\n",
    "    W_o -= learning_rate * dW_o\n",
    "    b_o -= learning_rate * db_o\n",
    "    W_y -= learning_rate * dW_y\n",
    "    b_y -= learning_rate * db_y\n",
    "    \n",
    "    print(f\"\\n  Applied gradients with learning rate {learning_rate} and clipping threshold {clip_threshold}\")\n",
    "    \n",
    "    # Print weight update summary\n",
    "    print(\"\\n  Weight Update Summary:\")\n",
    "    weight_table = [\n",
    "        [\"W_f\", f\"{weight_changes_epoch['W_f']:.6f}\"],\n",
    "        [\"W_i\", f\"{weight_changes_epoch['W_i']:.6f}\"],\n",
    "        [\"W_c\", f\"{weight_changes_epoch['W_c']:.6f}\"],\n",
    "        [\"W_o\", f\"{weight_changes_epoch['W_o']:.6f}\"],\n",
    "        [\"W_y\", f\"{weight_changes_epoch['W_y']:.6f}\"]\n",
    "    ]\n",
    "    print(tabulate(weight_table, headers=[\"Weight Matrix\", \"Change Magnitude\"], tablefmt=\"grid\"))\n",
    "\n",
    "# After all epochs, print the summary of training\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Loss progression\n",
    "print(\"\\nLoss Progression:\")\n",
    "loss_table = []\n",
    "for epoch, loss in enumerate(epoch_losses):\n",
    "    loss_table.append([epoch+1, f\"{loss:.6f}\"])\n",
    "print(tabulate(loss_table, headers=[\"Epoch\", \"Loss\"], tablefmt=\"grid\"))\n",
    "\n",
    "# Weight change progression\n",
    "print(\"\\nWeight Change Magnitudes:\")\n",
    "weight_change_table = []\n",
    "for epoch, changes in enumerate(weight_changes):\n",
    "    weight_change_table.append([\n",
    "        epoch+1, \n",
    "        f\"{changes['W_f']:.6f}\",\n",
    "        f\"{changes['W_i']:.6f}\",\n",
    "        f\"{changes['W_c']:.6f}\",\n",
    "        f\"{changes['W_o']:.6f}\",\n",
    "        f\"{changes['W_y']:.6f}\"\n",
    "    ])\n",
    "print(tabulate(weight_change_table, \n",
    "              headers=[\"Epoch\", \"W_f\", \"W_i\", \"W_c\", \"W_o\", \"W_y\"], \n",
    "              tablefmt=\"grid\"))\n",
    "\n",
    "# Weight comparison (initial vs final)\n",
    "print(\"\\nWeight Comparison (Initial vs Final):\")\n",
    "\n",
    "# Calculate relative change in each weight matrix\n",
    "def relative_change(initial, final):\n",
    "    return np.linalg.norm(final - initial) / np.linalg.norm(initial) * 100\n",
    "\n",
    "weight_comparison = [\n",
    "    [\"W_f\", f\"{np.linalg.norm(W_f_initial):.6f}\", f\"{np.linalg.norm(W_f):.6f}\", \n",
    "     f\"{relative_change(W_f_initial, W_f):.2f}%\"],\n",
    "    [\"W_i\", f\"{np.linalg.norm(W_i_initial):.6f}\", f\"{np.linalg.norm(W_i):.6f}\", \n",
    "     f\"{relative_change(W_i_initial, W_i):.2f}%\"],\n",
    "    [\"W_c\", f\"{np.linalg.norm(W_c_initial):.6f}\", f\"{np.linalg.norm(W_c):.6f}\", \n",
    "     f\"{relative_change(W_c_initial, W_c):.2f}%\"],\n",
    "    [\"W_o\", f\"{np.linalg.norm(W_o_initial):.6f}\", f\"{np.linalg.norm(W_o):.6f}\", \n",
    "     f\"{relative_change(W_o_initial, W_o):.2f}%\"],\n",
    "    [\"W_y\", f\"{np.linalg.norm(W_y_initial):.6f}\", f\"{np.linalg.norm(W_y):.6f}\", \n",
    "     f\"{relative_change(W_y_initial, W_y):.2f}%\"]\n",
    "]\n",
    "print(tabulate(weight_comparison, \n",
    "              headers=[\"Weight Matrix\", \"Initial Norm\", \"Final Norm\", \"Relative Change\"], \n",
    "              tablefmt=\"grid\"))\n",
    "\n",
    "print(\"\\nBackpropagation Through Time (BPTT) Key Insights:\")\n",
    "print(\"1. BPTT involves unfolding the network through time and computing gradients at each timestep\")\n",
    "print(\"2. The gradients from later timesteps flow back to earlier timesteps (temporal dependency)\")\n",
    "print(\"3. The cell state (C_t) acts as a memory mechanism, preserving information across timesteps\")\n",
    "print(\"4. The gates (f_t, i_t, o_t) control information flow and have their own gradient paths\")\n",
    "print(\"5. Gradient clipping helps prevent the exploding gradient problem in recurrent networks\")\n",
    "print(\"6. The learning process adjusts weights to minimize the prediction error over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14576854-28d9-434a-8d2b-181967c973ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521899c-f30a-4c79-b8be-9d1e19c4ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4587c5-9108-4738-8f2c-7be26123523a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext nb_js_diagrammers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16ede1-2297-496c-85bb-6c186c5456b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdca4e98-b3b4-4267-b0c0-97eb1d039bdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            graph RL\n",
       "    %% Output Layer\n",
       "    subgraph OL[Output Layer]\n",
       "        direction TB\n",
       "        L[Loss]\n",
       "        O2[y₂]\n",
       "        O1[y₁]\n",
       "    end\n",
       "\n",
       "    %% Hidden Layer\n",
       "    subgraph HL[Hidden Layer]\n",
       "        direction TB\n",
       "        H4[h₄]\n",
       "        H3[h₃]\n",
       "        H2[h₂]\n",
       "        H1[h₁]\n",
       "    end\n",
       "\n",
       "    %% Input Layer\n",
       "    subgraph IL[Input Layer]\n",
       "        direction TB\n",
       "        I1[x₁]\n",
       "        I2[x₂]\n",
       "    end\n",
       "\n",
       "    %% Backward flow arrows with gradient notations\n",
       "    L --&gt; O2\n",
       "    L --&gt; O1\n",
       "    \n",
       "    O2 --&gt;|∂L/∂W₂| H4\n",
       "    O2 --&gt;|∂L/∂W₂| H3\n",
       "    O2 --&gt;|∂L/∂W₂| H2\n",
       "    O2 --&gt;|∂L/∂W₂| H1\n",
       "    \n",
       "    O1 --&gt;|∂L/∂W₂| H4\n",
       "    O1 --&gt;|∂L/∂W₂| H3\n",
       "    O1 --&gt;|∂L/∂W₂| H2\n",
       "    O1 --&gt;|∂L/∂W₂| H1\n",
       "    \n",
       "    H4 --&gt;|∂L/∂W₁| I1\n",
       "    H4 --&gt;|∂L/∂W₁| I2\n",
       "    H3 --&gt;|∂L/∂W₁| I1\n",
       "    H3 --&gt;|∂L/∂W₁| I2\n",
       "    H2 --&gt;|∂L/∂W₁| I1\n",
       "    H2 --&gt;|∂L/∂W₁| I2\n",
       "    H1 --&gt;|∂L/∂W₁| I1\n",
       "    H1 --&gt;|∂L/∂W₁| I2\n",
       "\n",
       "    %% Add gradient flow annotations\n",
       "    annBP1[Gradient Flow:&lt;br&gt;Output → Hidden&lt;br&gt;∂L/∂W₂]\n",
       "    annBP2[Gradient Flow:&lt;br&gt;Hidden → Input&lt;br&gt;∂L/∂W₁]\n",
       "    annLoss[Cross-Entropy Loss&lt;br&gt;∂L/∂y]\n",
       "\n",
       "    annBP1 -.-&gt; HL\n",
       "    annBP2 -.-&gt; IL\n",
       "    annLoss -.-&gt; L\n",
       "\n",
       "    %% Styling\n",
       "    classDef inputClass fill:#f9f,stroke:#333,stroke-width:2px\n",
       "    classDef hiddenClass fill:#bbf,stroke:#333,stroke-width:2px\n",
       "    classDef outputClass fill:#bfb,stroke:#333,stroke-width:2px\n",
       "    classDef lossClass fill:#fbb,stroke:#333,stroke-width:2px\n",
       "    \n",
       "    class I1,I2 inputClass\n",
       "    class H1,H2,H3,H4 hiddenClass\n",
       "    class O1,O2 outputClass\n",
       "    class L lossClass\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"850\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x1203c5550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 850\n",
    "graph RL\n",
    "    %% Output Layer\n",
    "    subgraph OL[Output Layer]\n",
    "        direction TB\n",
    "        L[Loss]\n",
    "        O2[y₂]\n",
    "        O1[y₁]\n",
    "    end\n",
    "\n",
    "    %% Hidden Layer\n",
    "    subgraph HL[Hidden Layer]\n",
    "        direction TB\n",
    "        H4[h₄]\n",
    "        H3[h₃]\n",
    "        H2[h₂]\n",
    "        H1[h₁]\n",
    "    end\n",
    "\n",
    "    %% Input Layer\n",
    "    subgraph IL[Input Layer]\n",
    "        direction TB\n",
    "        I1[x₁]\n",
    "        I2[x₂]\n",
    "    end\n",
    "\n",
    "    %% Backward flow arrows with gradient notations\n",
    "    L --> O2\n",
    "    L --> O1\n",
    "    \n",
    "    O2 -->|∂L/∂W₂| H4\n",
    "    O2 -->|∂L/∂W₂| H3\n",
    "    O2 -->|∂L/∂W₂| H2\n",
    "    O2 -->|∂L/∂W₂| H1\n",
    "    \n",
    "    O1 -->|∂L/∂W₂| H4\n",
    "    O1 -->|∂L/∂W₂| H3\n",
    "    O1 -->|∂L/∂W₂| H2\n",
    "    O1 -->|∂L/∂W₂| H1\n",
    "    \n",
    "    H4 -->|∂L/∂W₁| I1\n",
    "    H4 -->|∂L/∂W₁| I2\n",
    "    H3 -->|∂L/∂W₁| I1\n",
    "    H3 -->|∂L/∂W₁| I2\n",
    "    H2 -->|∂L/∂W₁| I1\n",
    "    H2 -->|∂L/∂W₁| I2\n",
    "    H1 -->|∂L/∂W₁| I1\n",
    "    H1 -->|∂L/∂W₁| I2\n",
    "\n",
    "    %% Add gradient flow annotations\n",
    "    annBP1[Gradient Flow:<br>Output → Hidden<br>∂L/∂W₂]\n",
    "    annBP2[Gradient Flow:<br>Hidden → Input<br>∂L/∂W₁]\n",
    "    annLoss[Cross-Entropy Loss<br>∂L/∂y]\n",
    "\n",
    "    annBP1 -.-> HL\n",
    "    annBP2 -.-> IL\n",
    "    annLoss -.-> L\n",
    "\n",
    "    %% Styling\n",
    "    classDef inputClass fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    classDef hiddenClass fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    classDef outputClass fill:#bfb,stroke:#333,stroke-width:2px\n",
    "    classDef lossClass fill:#fbb,stroke:#333,stroke-width:2px\n",
    "    \n",
    "    class I1,I2 inputClass\n",
    "    class H1,H2,H3,H4 hiddenClass\n",
    "    class O1,O2 outputClass\n",
    "    class L lossClass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedacffb-0f5b-4aca-acf0-b21c241212d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde4ab45-7ce8-41ce-8f65-95bc7e036f48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "#             Forward Pass\n",
    "# ========================================================================\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sample_data(n_samples=6):\n",
    "    \"\"\"\n",
    "    Generate random sample data with 2 features\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "        X: Input features array of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    X = np.random.randint(1, 11, size=(n_samples, 2))\n",
    "    return X\n",
    "\n",
    "def initialize_weights(input_size=2, hidden_size=4, output_size=2):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for the neural network\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of nodes in hidden layer\n",
    "        output_size: Number of output nodes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing weights and biases\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        # Input to hidden layer weights\n",
    "        'W1': np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        # Hidden layer bias\n",
    "        'b1': np.zeros((1, hidden_size)),\n",
    "        # Hidden to output layer weights\n",
    "        'W2': np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        # Output layer bias\n",
    "        'b2': np.zeros((1, output_size))\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Args:\n",
    "        z: Input to the activation function\n",
    "        \n",
    "    Returns:\n",
    "        ReLU of input: max(0, z)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    \n",
    "    Args:\n",
    "        z: Input to the activation function, shape (batch_size, n_classes)\n",
    "        \n",
    "    Returns:\n",
    "        Softmax probabilities with same shape as input\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, params):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features array\n",
    "        params: Dictionary containing weights and biases\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing activations and intermediate values\n",
    "    \"\"\"\n",
    "    # Extract weights and biases\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    Z1 = np.dot(X, W1) + b1           # Linear transformation\n",
    "    A1 = relu(Z1)                     # ReLU Activation\n",
    "    \n",
    "    # Output layer calculations\n",
    "    Z2 = np.dot(A1, W2) + b2  # Linear transformation\n",
    "    A2 = softmax(Z2)        # Softmax activation\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1, 'A1': A1,\n",
    "        'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    \n",
    "    return cache\n",
    "\n",
    "\n",
    "def print_step_by_step(X, params, cache):\n",
    "    \"\"\"\n",
    "    Print detailed step-by-step calculations for feed forward neural network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        params: Network parameters\n",
    "        cache: Activation values from forward propagation\n",
    "    \"\"\"\n",
    "    print(\"\\nStep-by-Step Calculations:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Input layer\n",
    "    print(\"\\nInput Layer:\")\n",
    "    print(f\"Input features (X):\\n\\n{X}\")\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    print(\"\\nHidden Layer Calculations:\")\n",
    "    print(f\"Weights (W1):\\n\\n{params['W1']}\")\n",
    "    print(f\"Biases (b1):\\n\\n{params['b1']}\")\n",
    "    \n",
    "    print(f\"Weights (W1) shape: {params['W1'].shape}\")\n",
    "    print(f\"Biases (b1) shape: {params['b1'].shape}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear transformation (Z1 = X·W1 + b1):\\n\\n{cache['Z1']}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Activation (A1 = relu(Z1)):\\n\\n{cache['A1']}\")\n",
    "    print()\n",
    "    \n",
    "    # Output layer calculations\n",
    "    print(\"\\nOutput Layer Calculations:\")\n",
    "    print(f\"Weights (W2):\\n\\n{params['W2']}\")\n",
    "    print(f\"Biases (b2):\\n\\n{params['b2']}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear transformation (Z2 = A1·W2 + b2):\\n\\n{cache['Z2']}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Final Output (A2 = softmax(Z2)):\\n\\n{cache['A2']}\")\n",
    "\n",
    "def generate_target_data(n_samples=6, n_classes=2):\n",
    "    \"\"\"\n",
    "    Generate target variables for classification\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        n_classes: Number of classes (output nodes)\n",
    "        \n",
    "    Returns:\n",
    "        y: One-hot encoded target variables\n",
    "    \"\"\"\n",
    "    # Generate random class labels (0 to n_classes-1)\n",
    "    y_labels = np.random.randint(0, n_classes, size=n_samples)\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y = np.zeros((n_samples, n_classes))\n",
    "    y[np.arange(n_samples), y_labels] = 1\n",
    "    return y, y_labels\n",
    "\n",
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        params: Network parameters\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Predicted class labels\n",
    "        probabilities: Class probabilities\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    cache = forward_propagation(X, params)\n",
    "    \n",
    "    # Get probabilities from output layer\n",
    "    probabilities = cache['A2']\n",
    "    \n",
    "    # Get predicted class (argmax of probabilities)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true, probabilities):\n",
    "    \"\"\"\n",
    "    Print evaluation metrics for the predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted class labels\n",
    "        y_true: True class labels\n",
    "        probabilities: Predicted probabilities\n",
    "    \"\"\"\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nPredicted Probabilities:\")\n",
    "    print(probabilities)\n",
    "    print(\"\\nPredicted Classes:\", predictions)\n",
    "    print(\"True Classes:\", y_true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_true)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b613ced-a4b4-46c3-a8a7-600f63e0aa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178860dd-8206-4cb8-a8ef-5a6d51d4cc41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform forward propagation\n",
    "np.random.seed(1010)\n",
    "X = generate_sample_data()\n",
    "params = initialize_weights()\n",
    "cache = forward_propagation(X, params)\n",
    "# Generate target variables\n",
    "y, y_true = generate_target_data()\n",
    "\n",
    "# Make predictions\n",
    "predictions, probabilities = predict(X, params)\n",
    "\n",
    "# Evaluate predictions\n",
    "evaluate_predictions(predictions, y_true, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6bce2a-1662-472a-b676-a38a45d545cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted probabilities from softmax\n",
    "        y_true: True labels (one-hot encoded)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Cross-entropy loss value\n",
    "        d_loss: Gradient of loss with respect to predictions\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    # Gradient of cross-entropy loss with respect to predictions\n",
    "    d_loss = y_pred - y_true\n",
    "    \n",
    "    return loss, d_loss\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of ReLU activation\n",
    "    \n",
    "    Args:\n",
    "        Z: Input to ReLU function\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask where input was positive\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "\n",
    "def backward_propagation(X, y, params, cache):\n",
    "    \"\"\"\n",
    "    Perform backward propagation to compute gradients\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: True labels (one-hot encoded)\n",
    "        params: Network parameters\n",
    "        cache: Cached values from forward propagation\n",
    "        \n",
    "    Returns:\n",
    "        gradients: Dictionary containing gradients for all parameters\n",
    "    \"\"\"\n",
    "    # Get batch size\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Unpack parameters and cached values\n",
    "    W1, W2 = params['W1'], params['W2']\n",
    "    Z1, A1 = cache['Z1'], cache['A1']\n",
    "    Z2, A2 = cache['Z2'], cache['A2']\n",
    "    \n",
    "    # Step 1: Output layer gradients\n",
    "    # Compute loss gradient\n",
    "    _, dA2 = compute_loss(A2, y)            # dL/dA2\n",
    "    \n",
    "    # Step 2: Hidden to Output layer gradients\n",
    "    # dL/dZ2 = dL/dA2 (because softmax gradient is included in cross-entropy)\n",
    "    dZ2 = dA2\n",
    "    \n",
    "    # Compute gradients for W2 and b2\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)                   # dL/dW2\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # dL/db2\n",
    "    \n",
    "    # Step 3: Input to Hidden layer gradients\n",
    "    # Compute dL/dA1\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    \n",
    "    # Compute dL/dZ1 using ReLU derivative\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    \n",
    "    # Compute gradients for W1 and b1\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)  # dL/dW1\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # dL/db1\n",
    "    \n",
    "    # Store gradients in dictionary\n",
    "    gradients = {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "def update_parameters(params, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update network parameters using computed gradients\n",
    "    \n",
    "    Args:\n",
    "        params: Current network parameters\n",
    "        gradients: Computed gradients\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        Updated parameters\n",
    "    \"\"\"\n",
    "    # Update weights and biases\n",
    "    params['W1'] -= learning_rate * gradients['dW1']\n",
    "    params['b1'] -= learning_rate * gradients['db1']\n",
    "    params['W2'] -= learning_rate * gradients['dW2']\n",
    "    params['b2'] -= learning_rate * gradients['db2']\n",
    "    \n",
    "    return params\n",
    "\n",
    "def print_gradients(gradients):\n",
    "    \"\"\"\n",
    "    Print the computed gradients for inspection\n",
    "    \n",
    "    Args:\n",
    "        gradients: Dictionary containing computed gradients\n",
    "    \"\"\"\n",
    "    print(\"\\nGradient Information:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nGradients for W1 (Input → Hidden):\")\n",
    "    print(gradients['dW1'])\n",
    "    \n",
    "    print(\"\\nGradients for b1 (Hidden bias):\")\n",
    "    print(gradients['db1'])\n",
    "    \n",
    "    print(\"\\nGradients for W2 (Hidden → Output):\")\n",
    "    print(gradients['dW2'])\n",
    "    \n",
    "    print(\"\\nGradients for b2 (Output bias):\")\n",
    "    print(gradients['db2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff53b1a6-5c52-4815-ba1d-420c34efdf30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 0.6931\n",
      "\n",
      "Gradient Information:\n",
      "--------------------------------------------------\n",
      "\n",
      "Gradients for W1 (Input → Hidden):\n",
      "[[-0.00110582  0.          0.          0.        ]\n",
      " [-0.00110572  0.          0.          0.        ]]\n",
      "\n",
      "Gradients for b1 (Hidden bias):\n",
      "[[2.51779054e-08 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Gradients for W2 (Hidden → Output):\n",
      "[[-0.0001896  0.0001896]\n",
      " [ 0.         0.       ]\n",
      " [ 0.         0.       ]\n",
      " [ 0.         0.       ]]\n",
      "\n",
      "Gradients for b2 (Output bias):\n",
      "[[ 0.16666477 -0.16666477]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# Generate sample data\n",
    "X = generate_sample_data()\n",
    "y, _ = generate_target_data()\n",
    "    \n",
    "# Initialize parameters\n",
    "params = initialize_weights()\n",
    "\n",
    "# Forward propagation\n",
    "cache = forward_propagation(X, params)\n",
    "\n",
    "# Compute initial loss\n",
    "loss, _ = compute_loss(cache['A2'], y)\n",
    "print(f\"Initial loss: {loss:.4f}\")\n",
    "\n",
    "# Perform backward propagation\n",
    "gradients = backward_propagation(X, y, params, cache)\n",
    "\n",
    "# Print gradients\n",
    "print_gradients(gradients)\n",
    "\n",
    "# Update parameters\n",
    "learning_rate = 0.01\n",
    "params = update_parameters(params, gradients, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a883b6-e768-461a-98cc-883548de8a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b93d2-3775-42ce-b923-ee169a1e9070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbe5630-74bc-40a9-bf77-78f7be307354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a042bd-619e-486b-a5ca-433005b4614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

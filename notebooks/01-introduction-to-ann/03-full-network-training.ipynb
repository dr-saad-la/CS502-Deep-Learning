{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcfc173-1759-4269-a98f-5d92087f864c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521899c-f30a-4c79-b8be-9d1e19c4ddae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4587c5-9108-4738-8f2c-7be26123523a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%load_ext nb_js_diagrammers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae34938-ec31-4ac9-8099-2b7718b5f19c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdca4e98-b3b4-4267-b0c0-97eb1d039bdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe srcdoc=\"&lt;html&gt;\n",
       "    &lt;body&gt;\n",
       "        &lt;script src=&quot;https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js&quot;&gt;&lt;/script&gt;\n",
       "        &lt;script&gt;\n",
       "            mermaid.initialize({ startOnLoad: true });\n",
       "        &lt;/script&gt;\n",
       " \n",
       "        &lt;div class=&quot;mermaid&quot;&gt;\n",
       "            graph TD\n",
       "    %% Epoch Structure\n",
       "    Start[Start Training] --&gt; E1[Epoch 1]\n",
       "    E1 --&gt; FP1[Forward Pass]\n",
       "    FP1 --&gt; L1[Calculate Loss]\n",
       "    L1 --&gt; BP1[Backward Pass]\n",
       "    BP1 --&gt; U1[Update Weights]\n",
       "    U1 --&gt; E2[Epoch 2]\n",
       "    \n",
       "    E2 --&gt; FP2[Forward Pass]\n",
       "    FP2 --&gt; L2[Calculate Loss]\n",
       "    L2 --&gt; BP2[Backward Pass]\n",
       "    BP2 --&gt; U2[Update Weights]\n",
       "    U2 --&gt; E3[Epoch 3]\n",
       "    \n",
       "    E3 --&gt; FP3[Forward Pass]\n",
       "    FP3 --&gt; L3[Calculate Loss]\n",
       "    L3 --&gt; BP3[Backward Pass]\n",
       "    BP3 --&gt; U3[Update Weights]\n",
       "    U3 --&gt; End[End Training]\n",
       "    \n",
       "    %% Add information for each epoch\n",
       "    EI1[Record:&lt;br&gt;Weights&lt;br&gt;Biases&lt;br&gt;Loss] --- E1\n",
       "    EI2[Record:&lt;br&gt;Weights&lt;br&gt;Biases&lt;br&gt;Loss] --- E2\n",
       "    EI3[Record:&lt;br&gt;Weights&lt;br&gt;Biases&lt;br&gt;Loss] --- E3\n",
       "    \n",
       "    %% Styling\n",
       "    classDef epoch fill:#f9f,stroke:#333,stroke-width:2px\n",
       "    classDef process fill:#bbf,stroke:#333,stroke-width:2px\n",
       "    classDef info fill:#bfb,stroke:#333,stroke-width:2px\n",
       "    classDef endpoint fill:#fbb,stroke:#333,stroke-width:2px\n",
       "    \n",
       "    class E1,E2,E3 epoch\n",
       "    class FP1,FP2,FP3,BP1,BP2,BP3,L1,L2,L3,U1,U2,U3 process\n",
       "    class EI1,EI2,EI3 info\n",
       "    class Start,End endpoint\n",
       "        &lt;/div&gt;\n",
       " \n",
       "    &lt;/body&gt;\n",
       "&lt;/html&gt;\n",
       "\" width=\"100%\" height=\"250\"style=\"border:none !important;\" \"allowfullscreen\" \"webkitallowfullscreen\" \"mozallowfullscreen\"></iframe>"
      ],
      "text/plain": [
       "<nb_js_diagrammers.magics.JSDiagram at 0x105c36390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%mermaid_magic -h 250\n",
    "graph TD\n",
    "    %% Epoch Structure\n",
    "    Start[Start Training] --> E1[Epoch 1]\n",
    "    E1 --> FP1[Forward Pass]\n",
    "    FP1 --> L1[Calculate Loss]\n",
    "    L1 --> BP1[Backward Pass]\n",
    "    BP1 --> U1[Update Weights]\n",
    "    U1 --> E2[Epoch 2]\n",
    "    \n",
    "    E2 --> FP2[Forward Pass]\n",
    "    FP2 --> L2[Calculate Loss]\n",
    "    L2 --> BP2[Backward Pass]\n",
    "    BP2 --> U2[Update Weights]\n",
    "    U2 --> E3[Epoch 3]\n",
    "    \n",
    "    E3 --> FP3[Forward Pass]\n",
    "    FP3 --> L3[Calculate Loss]\n",
    "    L3 --> BP3[Backward Pass]\n",
    "    BP3 --> U3[Update Weights]\n",
    "    U3 --> End[End Training]\n",
    "    \n",
    "    %% Add information for each epoch\n",
    "    EI1[Record:<br>Weights<br>Biases<br>Loss] --- E1\n",
    "    EI2[Record:<br>Weights<br>Biases<br>Loss] --- E2\n",
    "    EI3[Record:<br>Weights<br>Biases<br>Loss] --- E3\n",
    "    \n",
    "    %% Styling\n",
    "    classDef epoch fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    classDef process fill:#bbf,stroke:#333,stroke-width:2px\n",
    "    classDef info fill:#bfb,stroke:#333,stroke-width:2px\n",
    "    classDef endpoint fill:#fbb,stroke:#333,stroke-width:2px\n",
    "    \n",
    "    class E1,E2,E3 epoch\n",
    "    class FP1,FP2,FP3,BP1,BP2,BP3,L1,L2,L3,U1,U2,U3 process\n",
    "    class EI1,EI2,EI3 info\n",
    "    class Start,End endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedacffb-0f5b-4aca-acf0-b21c241212d9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde4ab45-7ce8-41ce-8f65-95bc7e036f48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "#             Forward Pass\n",
    "# ========================================================================\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sample_data(n_samples=6):\n",
    "    \"\"\"\n",
    "    Generate random sample data with 2 features\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "        X: Input features array of shape (n_samples, 2)\n",
    "    \"\"\"\n",
    "    X = np.random.randint(1, 11, size=(n_samples, 2))\n",
    "    return X\n",
    "\n",
    "def initialize_weights(input_size=2, hidden_size=4, output_size=2):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for the neural network\n",
    "    \n",
    "    Args:\n",
    "        input_size: Number of input features\n",
    "        hidden_size: Number of nodes in hidden layer\n",
    "        output_size: Number of output nodes\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing weights and biases\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        # Input to hidden layer weights\n",
    "        'W1': np.random.randn(input_size, hidden_size) * 0.01,\n",
    "        # Hidden layer bias\n",
    "        'b1': np.zeros((1, hidden_size)),\n",
    "        # Hidden to output layer weights\n",
    "        'W2': np.random.randn(hidden_size, output_size) * 0.01,\n",
    "        # Output layer bias\n",
    "        'b2': np.zeros((1, output_size))\n",
    "    }\n",
    "    return params\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "    \n",
    "    Args:\n",
    "        z: Input to the activation function\n",
    "        \n",
    "    Returns:\n",
    "        ReLU of input: max(0, z)\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Softmax activation function\n",
    "    \n",
    "    Args:\n",
    "        z: Input to the activation function, shape (batch_size, n_classes)\n",
    "        \n",
    "    Returns:\n",
    "        Softmax probabilities with same shape as input\n",
    "    \"\"\"\n",
    "    # Subtract max for numerical stability (prevents overflow)\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forward_propagation(X, params):\n",
    "    \"\"\"\n",
    "    Perform forward propagation through the network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features array\n",
    "        params: Dictionary containing weights and biases\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing activations and intermediate values\n",
    "    \"\"\"\n",
    "    # Extract weights and biases\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    Z1 = np.dot(X, W1) + b1           # Linear transformation\n",
    "    A1 = relu(Z1)                     # ReLU Activation\n",
    "    \n",
    "    # Output layer calculations\n",
    "    Z2 = np.dot(A1, W2) + b2  # Linear transformation\n",
    "    A2 = softmax(Z2)        # Softmax activation\n",
    "    \n",
    "    cache = {\n",
    "        'Z1': Z1, 'A1': A1,\n",
    "        'Z2': Z2, 'A2': A2\n",
    "    }\n",
    "    \n",
    "    return cache\n",
    "\n",
    "\n",
    "def print_step_by_step(X, params, cache):\n",
    "    \"\"\"\n",
    "    Print detailed step-by-step calculations for feed forward neural network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        params: Network parameters\n",
    "        cache: Activation values from forward propagation\n",
    "    \"\"\"\n",
    "    print(\"\\nStep-by-Step Calculations:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Input layer\n",
    "    print(\"\\nInput Layer:\")\n",
    "    print(f\"Input features (X):\\n\\n{X}\")\n",
    "    \n",
    "    # Hidden layer calculations\n",
    "    print(\"\\nHidden Layer Calculations:\")\n",
    "    print(f\"Weights (W1):\\n\\n{params['W1']}\")\n",
    "    print(f\"Biases (b1):\\n\\n{params['b1']}\")\n",
    "    \n",
    "    print(f\"Weights (W1) shape: {params['W1'].shape}\")\n",
    "    print(f\"Biases (b1) shape: {params['b1'].shape}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear transformation (Z1 = XÂ·W1 + b1):\\n\\n{cache['Z1']}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Activation (A1 = relu(Z1)):\\n\\n{cache['A1']}\")\n",
    "    print()\n",
    "    \n",
    "    # Output layer calculations\n",
    "    print(\"\\nOutput Layer Calculations:\")\n",
    "    print(f\"Weights (W2):\\n\\n{params['W2']}\")\n",
    "    print(f\"Biases (b2):\\n\\n{params['b2']}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"Linear transformation (Z2 = A1Â·W2 + b2):\\n\\n{cache['Z2']}\")\n",
    "    \n",
    "    print()\n",
    "    print(f\"Final Output (A2 = softmax(Z2)):\\n\\n{cache['A2']}\")\n",
    "\n",
    "def generate_target_data(n_samples=6, n_classes=2):\n",
    "    \"\"\"\n",
    "    Generate target variables for classification\n",
    "    \n",
    "    Args:\n",
    "        n_samples: Number of samples\n",
    "        n_classes: Number of classes (output nodes)\n",
    "        \n",
    "    Returns:\n",
    "        y: One-hot encoded target variables\n",
    "    \"\"\"\n",
    "    # Generate random class labels (0 to n_classes-1)\n",
    "    y_labels = np.random.randint(0, n_classes, size=n_samples)\n",
    "    \n",
    "    # Convert to one-hot encoding\n",
    "    y = np.zeros((n_samples, n_classes))\n",
    "    y[np.arange(n_samples), y_labels] = 1\n",
    "    return y, y_labels\n",
    "\n",
    "def predict(X, params):\n",
    "    \"\"\"\n",
    "    Make predictions using the trained network\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        params: Network parameters\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Predicted class labels\n",
    "        probabilities: Class probabilities\n",
    "    \"\"\"\n",
    "    # Forward pass\n",
    "    cache = forward_propagation(X, params)\n",
    "    \n",
    "    # Get probabilities from output layer\n",
    "    probabilities = cache['A2']\n",
    "    \n",
    "    # Get predicted class (argmax of probabilities)\n",
    "    predictions = np.argmax(probabilities, axis=1)\n",
    "    \n",
    "    return predictions, probabilities\n",
    "\n",
    "\n",
    "def evaluate_predictions(predictions, y_true, probabilities):\n",
    "    \"\"\"\n",
    "    Print evaluation metrics for the predictions\n",
    "    \n",
    "    Args:\n",
    "        predictions: Predicted class labels\n",
    "        y_true: True class labels\n",
    "        probabilities: Predicted probabilities\n",
    "    \"\"\"\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nPredicted Probabilities:\")\n",
    "    print(probabilities)\n",
    "    print(\"\\nPredicted Classes:\", predictions)\n",
    "    print(\"True Classes:\", y_true)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_true)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b613ced-a4b4-46c3-a8a7-600f63e0aa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb6bce2a-1662-472a-b676-a38a45d545cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "#             Backworkd Pass\n",
    "# ========================================================================\n",
    "def compute_loss(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute cross-entropy loss\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted probabilities from softmax\n",
    "        y_true: True labels (one-hot encoded)\n",
    "        \n",
    "    Returns:\n",
    "        loss: Cross-entropy loss value\n",
    "        d_loss: Gradient of loss with respect to predictions\n",
    "    \"\"\"\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Compute cross-entropy loss\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "    \n",
    "    # Gradient of cross-entropy loss with respect to predictions\n",
    "    d_loss = y_pred - y_true\n",
    "    \n",
    "    return loss, d_loss\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    \"\"\"\n",
    "    Compute derivative of ReLU activation\n",
    "    \n",
    "    Args:\n",
    "        Z: Input to ReLU function\n",
    "        \n",
    "    Returns:\n",
    "        Binary mask where input was positive\n",
    "    \"\"\"\n",
    "    return Z > 0\n",
    "\n",
    "def backward_propagation(X, y, params, cache):\n",
    "    \"\"\"\n",
    "    Perform backward propagation to compute gradients\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: True labels (one-hot encoded)\n",
    "        params: Network parameters\n",
    "        cache: Cached values from forward propagation\n",
    "        \n",
    "    Returns:\n",
    "        gradients: Dictionary containing gradients for all parameters\n",
    "    \"\"\"\n",
    "    # Get batch size\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Unpack parameters and cached values\n",
    "    W1, W2 = params['W1'], params['W2']\n",
    "    Z1, A1 = cache['Z1'], cache['A1']\n",
    "    Z2, A2 = cache['Z2'], cache['A2']\n",
    "    \n",
    "    # Step 1: Output layer gradients\n",
    "    # Compute loss gradient\n",
    "    _, dA2 = compute_loss(A2, y)            # dL/dA2\n",
    "    \n",
    "    # Step 2: Hidden to Output layer gradients\n",
    "    # dL/dZ2 = dL/dA2 (because softmax gradient is included in cross-entropy)\n",
    "    dZ2 = dA2\n",
    "    \n",
    "    # Compute gradients for W2 and b2\n",
    "    dW2 = (1/m) * np.dot(A1.T, dZ2)                   # dL/dW2\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=0, keepdims=True)  # dL/db2\n",
    "    \n",
    "    # Step 3: Input to Hidden layer gradients\n",
    "    # Compute dL/dA1\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    \n",
    "    # Compute dL/dZ1 using ReLU derivative\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    \n",
    "    # Compute gradients for W1 and b1\n",
    "    dW1 = (1/m) * np.dot(X.T, dZ1)  # dL/dW1\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=0, keepdims=True)  # dL/db1\n",
    "    \n",
    "    # Store gradients in dictionary\n",
    "    gradients = {\n",
    "        'dW1': dW1, 'db1': db1,\n",
    "        'dW2': dW2, 'db2': db2\n",
    "    }\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(params, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update network parameters using computed gradients\n",
    "    \n",
    "    Args:\n",
    "        params: Current network parameters\n",
    "        gradients: Computed gradients\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "        \n",
    "    Returns:\n",
    "        Updated parameters\n",
    "    \"\"\"\n",
    "    # Update weights and biases\n",
    "    params['W1'] -= learning_rate * gradients['dW1']\n",
    "    params['b1'] -= learning_rate * gradients['db1']\n",
    "    params['W2'] -= learning_rate * gradients['dW2']\n",
    "    params['b2'] -= learning_rate * gradients['db2']\n",
    "    \n",
    "    return params\n",
    "\n",
    "def print_gradients(gradients):\n",
    "    \"\"\"\n",
    "    Print the computed gradients for inspection\n",
    "    \n",
    "    Args:\n",
    "        gradients: Dictionary containing computed gradients\n",
    "    \"\"\"\n",
    "    print(\"\\nGradient Information:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nGradients for W1 (Input â Hidden):\")\n",
    "    print(gradients['dW1'])\n",
    "    \n",
    "    print(\"\\nGradients for b1 (Hidden bias):\")\n",
    "    print(gradients['db1'])\n",
    "    \n",
    "    print(\"\\nGradients for W2 (Hidden â Output):\")\n",
    "    print(gradients['dW2'])\n",
    "    \n",
    "    print(\"\\nGradients for b2 (Output bias):\")\n",
    "    print(gradients['db2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e870190b-13a2-4838-ae9f-7191c380adb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========================================================================\n",
    "#             Training Neural Network\n",
    "# ========================================================================\n",
    "def train_network(X, y, params, n_epochs=3, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Train the neural network for specified number of epochs\n",
    "    \n",
    "    Args:\n",
    "        X: Input features\n",
    "        y: Target variables (one-hot encoded)\n",
    "        params: Initial network parameters\n",
    "        n_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "        params: Updated parameters\n",
    "        history: Training history\n",
    "    \"\"\"\n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'params': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{n_epochs}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Forward propagation\n",
    "        cache = forward_propagation(X, params)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss, _ = compute_loss(cache['A2'], y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = backward_propagation(X, y, params, cache)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, gradients, learning_rate)\n",
    "        \n",
    "        # Store history\n",
    "        history['loss'].append(loss)\n",
    "        history['params'].append(params.copy())\n",
    "        \n",
    "        # Print epoch details\n",
    "        print_epoch_details(params, cache, loss)\n",
    "    \n",
    "    return params, history\n",
    "\n",
    "def print_epoch_details(params, cache, loss):\n",
    "    \"\"\"\n",
    "    Print detailed information for current epoch\n",
    "    \"\"\"\n",
    "    print(\"\\nNetwork Parameters:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"\\nWeights (W1) - Input to Hidden:\")\n",
    "    print(params['W1'])\n",
    "    print(\"\\nBiases (b1) - Hidden Layer:\")\n",
    "    print(params['b1'])\n",
    "    print(\"\\nWeights (W2) - Hidden to Output:\")\n",
    "    print(params['W2'])\n",
    "    print(\"\\nBiases (b2) - Output Layer:\")\n",
    "    print(params['b2'])\n",
    "    \n",
    "    print(\"\\nNetwork Outputs:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"\\nHidden Layer Activations (A1):\")\n",
    "    print(cache['A1'])\n",
    "    print(\"\\nOutput Layer Activations (A2):\")\n",
    "    print(cache['A2'])\n",
    "    \n",
    "    print(\"\\nLoss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e269398-4509-4fc9-9da2-4f2d83423f46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial State:\n",
      "==================================================\n",
      "\n",
      "Network Parameters:\n",
      "-------------------------\n",
      "\n",
      "Weights (W1) - Input to Hidden:\n",
      "[[-0.00469474  0.0054256  -0.00463418 -0.0046573 ]\n",
      " [ 0.00241962 -0.0191328  -0.01724918 -0.00562288]]\n",
      "\n",
      "Biases (b1) - Hidden Layer:\n",
      "[[0. 0. 0. 0.]]\n",
      "\n",
      "Weights (W2) - Hidden to Output:\n",
      "[[-0.01012831  0.00314247]\n",
      " [-0.00908024 -0.01412304]\n",
      " [ 0.01465649 -0.00225776]\n",
      " [ 0.00067528 -0.01424748]]\n",
      "\n",
      "Biases (b2) - Output Layer:\n",
      "[[0. 0.]]\n",
      "\n",
      "Network Outputs:\n",
      "-------------------------\n",
      "\n",
      "Hidden Layer Activations (A1):\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00285313 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00057801 0.         0.         0.        ]]\n",
      "\n",
      "Output Layer Activations (A2):\n",
      "[[0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.49999053 0.50000947]\n",
      " [0.5        0.5       ]\n",
      " [0.49999808 0.50000192]]\n",
      "\n",
      "Loss: 0.6931496966445779\n",
      "\n",
      "Epoch 1/3\n",
      "==================================================\n",
      "\n",
      "Network Parameters:\n",
      "-------------------------\n",
      "\n",
      "Weights (W1) - Input to Hidden:\n",
      "[[-0.00468369  0.0054256  -0.00463418 -0.0046573 ]\n",
      " [ 0.00243068 -0.0191328  -0.01724918 -0.00562288]]\n",
      "\n",
      "Biases (b1) - Hidden Layer:\n",
      "[[-2.51779054e-10  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "Weights (W2) - Hidden to Output:\n",
      "[[-0.01012642  0.00314058]\n",
      " [-0.00908024 -0.01412304]\n",
      " [ 0.01465649 -0.00225776]\n",
      " [ 0.00067528 -0.01424748]]\n",
      "\n",
      "Biases (b2) - Output Layer:\n",
      "[[-0.00166665  0.00166665]]\n",
      "\n",
      "Network Outputs:\n",
      "-------------------------\n",
      "\n",
      "Hidden Layer Activations (A1):\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00285313 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00057801 0.         0.         0.        ]]\n",
      "\n",
      "Output Layer Activations (A2):\n",
      "[[0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.5        0.5       ]\n",
      " [0.49999053 0.50000947]\n",
      " [0.5        0.5       ]\n",
      " [0.49999808 0.50000192]]\n",
      "\n",
      "Loss: 0.6931496966445779\n",
      "\n",
      "Epoch 2/3\n",
      "==================================================\n",
      "\n",
      "Network Parameters:\n",
      "-------------------------\n",
      "\n",
      "Weights (W1) - Input to Hidden:\n",
      "[[-0.00467276  0.0054256  -0.00463418 -0.0046573 ]\n",
      " [ 0.00244146 -0.0191328  -0.01724918 -0.00562288]]\n",
      "\n",
      "Biases (b1) - Hidden Layer:\n",
      "[[-3.73735577e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "Weights (W2) - Hidden to Output:\n",
      "[[-0.01012453  0.00313869]\n",
      " [-0.00908024 -0.01412304]\n",
      " [ 0.01465649 -0.00225776]\n",
      " [ 0.00067528 -0.01424748]]\n",
      "\n",
      "Biases (b2) - Output Layer:\n",
      "[[-0.00332496  0.00332496]]\n",
      "\n",
      "Network Outputs:\n",
      "-------------------------\n",
      "\n",
      "Hidden Layer Activations (A1):\n",
      "[[0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.0029637 0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.0007107 0.        0.        0.       ]]\n",
      "\n",
      "Output Layer Activations (A2):\n",
      "[[0.49916668 0.50083332]\n",
      " [0.49916668 0.50083332]\n",
      " [0.49916668 0.50083332]\n",
      " [0.49915685 0.50084315]\n",
      " [0.49916668 0.50083332]\n",
      " [0.49916432 0.50083568]]\n",
      "\n",
      "Loss: 0.6925955178739578\n",
      "\n",
      "Epoch 3/3\n",
      "==================================================\n",
      "\n",
      "Network Parameters:\n",
      "-------------------------\n",
      "\n",
      "Weights (W1) - Input to Hidden:\n",
      "[[-0.00466197  0.0054256  -0.00463418 -0.0046573 ]\n",
      " [ 0.00245196 -0.0191328  -0.01724918 -0.00562288]]\n",
      "\n",
      "Biases (b1) - Hidden Layer:\n",
      "[[-1.11159575e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n",
      "\n",
      "Weights (W2) - Hidden to Output:\n",
      "[[-0.01012266  0.00313682]\n",
      " [-0.00908024 -0.01412304]\n",
      " [ 0.01465649 -0.00225776]\n",
      " [ 0.00067528 -0.01424748]]\n",
      "\n",
      "Biases (b2) - Output Layer:\n",
      "[[-0.00497498  0.00497498]]\n",
      "\n",
      "Network Outputs:\n",
      "-------------------------\n",
      "\n",
      "Hidden Layer Activations (A1):\n",
      "[[0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00307189 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00084058 0.         0.         0.        ]]\n",
      "\n",
      "Output Layer Activations (A2):\n",
      "[[0.49833753 0.50166247]\n",
      " [0.49833753 0.50166247]\n",
      " [0.49833753 0.50166247]\n",
      " [0.49832734 0.50167266]\n",
      " [0.49833753 0.50166247]\n",
      " [0.49833474 0.50166526]]\n",
      "\n",
      "Loss: 0.6920468685636264\n",
      "\n",
      "Training Complete!\n",
      "==================================================\n",
      "Initial Loss: 0.6931\n",
      "Final Loss: 0.6920\n",
      "\n",
      "Loss progression over epochs:\n",
      "Epoch 1: 0.6931\n",
      "Epoch 2: 0.6926\n",
      "Epoch 3: 0.6920\n"
     ]
    }
   ],
   "source": [
    "# Training NNet\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "X = generate_sample_data()\n",
    "y, y_true = generate_target_data()\n",
    "\n",
    "# Initialize parameters\n",
    "params = initialize_weights()\n",
    "\n",
    "# Print initial state\n",
    "print(\"\\nInitial State:\")\n",
    "print(\"=\" * 50)\n",
    "cache = forward_propagation(X, params)\n",
    "initial_loss, _ = compute_loss(cache['A2'], y)\n",
    "print_epoch_details(params, cache, initial_loss)\n",
    "\n",
    "# Train for 3 epochs\n",
    "final_params, history = train_network(X, y, params, n_epochs=3)\n",
    "\n",
    "# Print final loss\n",
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Initial Loss: {initial_loss:.4f}\")\n",
    "print(f\"Final Loss: {history['loss'][-1]:.4f}\")\n",
    "\n",
    "# Print loss progression\n",
    "print(\"\\nLoss progression over epochs:\")\n",
    "for epoch, loss in enumerate(history['loss']):\n",
    "    print(f\"Epoch {epoch + 1}: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a042bd-619e-486b-a5ca-433005b4614d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU:2.16",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
